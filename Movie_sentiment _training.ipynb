{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\train\\pos\n",
      "Loading data from C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\train\\neg\n",
      "Loading data from C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\test\\pos\n",
      "Loading data from C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\test\\neg\n"
     ]
    }
   ],
   "source": [
    "# Path to the dataset\n",
    "train_dir= r'C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\train'\n",
    "test_dir = r'C:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\aclImdb_v1\\aclImdb\\test'\n",
    "\n",
    "def load_data(data_dir):\n",
    "    data = []\n",
    "    labels = []\n",
    "    for label in ['pos', 'neg']:\n",
    "        folder = os.path.join(data_dir, label)\n",
    "        print(f\"Loading data from {folder}\")  # Debug print\n",
    "        for filename in os.listdir(folder):\n",
    "            with open(os.path.join(folder, filename), 'r', encoding='utf-8') as file:\n",
    "                data.append(file.read())\n",
    "            labels.append(1 if label == 'pos' else 0)\n",
    "    return data, labels\n",
    "\n",
    "# Load train and test data\n",
    "train_data, train_labels = load_data(train_dir)\n",
    "test_data, test_labels = load_data(test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a validation set from training data\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_data, train_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation Accuracy:  0.8482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85      2485\n",
      "           1       0.85      0.84      0.85      2515\n",
      "\n",
      "    accuracy                           0.85      5000\n",
      "   macro avg       0.85      0.85      0.85      5000\n",
      "weighted avg       0.85      0.85      0.85      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using TF-IDF Vectorizer\n",
    "tfidf_vectorizer= TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(train_texts)\n",
    "X_val_tfidf = tfidf_vectorizer.transform(val_texts)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(test_data)\n",
    "\n",
    "# Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predictions\n",
    "nb_preds = nb_model.predict(X_val_tfidf)\n",
    "print(\"Naive Bayes Validation Accuracy: \", accuracy_score(val_labels, nb_preds))\n",
    "print(classification_report(val_labels, nb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation Accuracy:  0.8878\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89      2485\n",
      "           1       0.88      0.90      0.89      2515\n",
      "\n",
      "    accuracy                           0.89      5000\n",
      "   macro avg       0.89      0.89      0.89      5000\n",
      "weighted avg       0.89      0.89      0.89      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression model with L2 regularization\n",
    "lr_model = LogisticRegression(max_iter=1000)\n",
    "lr_model.fit(X_train_tfidf, train_labels)\n",
    "\n",
    "# Predictions\n",
    "lr_preds = lr_model.predict(X_val_tfidf)\n",
    "print(\"Logistic Regression Validation Accuracy: \", accuracy_score(val_labels, lr_preds))\n",
    "print(classification_report(val_labels, lr_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: LSTM (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Isi\\Desktop\\datanerds\\Project_1\\Movie_Sentiment_Project\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenize text for LSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_texts)\n",
    "X_val_seq = tokenizer.texts_to_sequences(val_texts)\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "\n",
    "# Padding sequences to have the same length\n",
    "maxlen = 200\n",
    "X_train_seq = pad_sequences(X_train_seq, maxlen=maxlen)\n",
    "X_val_seq = pad_sequences(X_val_seq, maxlen=maxlen)\n",
    "X_test_seq = pad_sequences(X_test_seq, maxlen=maxlen)\n",
    "\n",
    "# Build LSTM Model\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=5000, output_dim=128, input_length=maxlen),\n",
    "    tf.keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 351ms/step - accuracy: 0.6999 - loss: 0.5650 - val_accuracy: 0.8098 - val_loss: 0.4229\n",
      "Epoch 2/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 348ms/step - accuracy: 0.8511 - loss: 0.3592 - val_accuracy: 0.8208 - val_loss: 0.3921\n",
      "Epoch 3/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 344ms/step - accuracy: 0.8671 - loss: 0.3229 - val_accuracy: 0.7852 - val_loss: 0.4435\n",
      "Epoch 4/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 340ms/step - accuracy: 0.8598 - loss: 0.3322 - val_accuracy: 0.8508 - val_loss: 0.3652\n",
      "Epoch 5/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 348ms/step - accuracy: 0.9018 - loss: 0.2461 - val_accuracy: 0.8404 - val_loss: 0.3831\n",
      "Epoch 6/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 340ms/step - accuracy: 0.9178 - loss: 0.2136 - val_accuracy: 0.8560 - val_loss: 0.3601\n",
      "Epoch 7/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 341ms/step - accuracy: 0.9322 - loss: 0.1821 - val_accuracy: 0.8412 - val_loss: 0.4346\n",
      "Epoch 8/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 334ms/step - accuracy: 0.9255 - loss: 0.1934 - val_accuracy: 0.8620 - val_loss: 0.4178\n",
      "Epoch 9/10\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 338ms/step - accuracy: 0.9442 - loss: 0.1486 - val_accuracy: 0.8520 - val_loss: 0.4496\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 69ms/step\n",
      "LSTM Validation Accuracy:  0.852\n"
     ]
    }
   ],
   "source": [
    "# Train the LSTM model\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "lstm_model.fit(X_train_seq, np.array(train_labels), epochs=10, batch_size=64, validation_data=(X_val_seq, np.array(val_labels)), callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "lstm_preds = (lstm_model.predict(X_val_seq) > 0.5).astype(\"int32\")\n",
    "print(\"LSTM Validation Accuracy: \", accuracy_score(val_labels, lstm_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Test Accuracy:  0.84048\n",
      "Logistic Regression Test Accuracy:  0.879\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 59ms/step\n",
      "LSTM Test Accuracy:  0.84656\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test data\n",
    "nb_test_preds = nb_model.predict(X_test_tfidf)\n",
    "print(\"Naive Bayes Test Accuracy: \", accuracy_score(test_labels, nb_test_preds))\n",
    "\n",
    "lr_test_preds = lr_model.predict(X_test_tfidf)\n",
    "print(\"Logistic Regression Test Accuracy: \", accuracy_score(test_labels, lr_test_preds))\n",
    "\n",
    "lstm_test_preds = (lstm_model.predict(X_test_seq) > 0.5).astype(\"int32\")\n",
    "print(\"LSTM Test Accuracy: \", accuracy_score(test_labels, lstm_test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_regression_model.joblib']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(lr_model, 'logistic_regression_model.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.joblib']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the vectorizer\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
